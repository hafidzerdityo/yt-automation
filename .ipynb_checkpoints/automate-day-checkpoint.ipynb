{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attached-louisville",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T04:44:52.628588Z",
     "start_time": "2022-04-17T04:44:52.621370Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def bot_msg_hafidz(text):\n",
    "    import requests\n",
    "    url = \"https://api.telegram.org/bot5332835402%3AAAH7YQ4ru9mDvg9opYyYVYoWjertyPnA5rI/sendMessage\"\n",
    "\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"disable_web_page_preview\": False,\n",
    "        \"disable_notification\": False,\n",
    "        \"reply_to_message_id\": None,\n",
    "        \"chat_id\": \"1908911926\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, json=payload, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "roman-estimate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T04:44:55.502974Z",
     "start_time": "2022-04-17T04:44:55.460459Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "def crawling_harian():\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    # ser = Service()\n",
    "    op = webdriver.ChromeOptions()\n",
    "    op.add_argument('headless')\n",
    "    driver = webdriver.Chrome(options=op)\n",
    "    \n",
    "    bot_msg_hafidz('Youtube Crawling Started!')\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"%m/%d/%Y\").replace('/','-')\n",
    "    \n",
    "    print('Starting to get all link...=================================================================\\n')\n",
    "    bot_msg_hafidz('Starting to get all link...')\n",
    "    links = []\n",
    "    channel_name = []\n",
    "    days_list = ['21 hours ago','22 hours ago','23 hours ago','1 day ago']\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import time\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from tqdm import tqdm\n",
    "\n",
    "#     channels = \"\"\"CNNindonesiaOfficial\n",
    "# kompastv\n",
    "# tribunnews\n",
    "# tvOneNews\n",
    "# MetrotvnewsOfficial\n",
    "# BeritaSatuChannel\n",
    "# VIVAcoid\n",
    "# CNBCIndonesia_ID\n",
    "# OfficialiNews\n",
    "# HarianKompasCetak\n",
    "# mykompascom\n",
    "# UCQA6NejSxQguRkD3L8eXHzA\n",
    "# OfficialNETNews\n",
    "# TribunMedanTV\n",
    "# detikcom\n",
    "# TribunJogjaOfficial\n",
    "# PikiranRakyatDigital\n",
    "# suaradotcom\"\"\"\n",
    "\n",
    "#     channels_decode = \"\"\"CNNindonesiaOfficial\n",
    "# kompastv\n",
    "# tribunnews\n",
    "# tvOneNews\n",
    "# MetrotvnewsOfficial\n",
    "# BeritaSatuChannel\n",
    "# VIVAcoid\n",
    "# CNBCIndonesia_ID\n",
    "# OfficialiNews\n",
    "# HarianKompasCetak\n",
    "# kompas.com\n",
    "# iNews_id\n",
    "# OfficialNETNews\n",
    "# TribunMedanTV\n",
    "# detikcom\n",
    "# TribunJogjaOfficial\n",
    "# PikiranRakyatDigital\n",
    "# suaradotcom\"\"\"\n",
    "    \n",
    "    channels = \"\"\"CNNindonesiaOfficial\n",
    "kompastv\"\"\"\n",
    "\n",
    "    channels_decode = \"\"\"CNNindonesiaOfficial\n",
    "kompastv\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    channels = channels.split('\\n')\n",
    "    channels_decode = channels_decode.split('\\n')\n",
    "\n",
    "    for channel,decode in zip(tqdm(channels), channels_decode):\n",
    "        start_time = time.time()\n",
    "        if channel in ['UCQA6NejSxQguRkD3L8eXHzA','UCH_ElasO_yPy0WI3rAOUkQQ','UCpFqnctVbdqj1UketjDVz4Q']:\n",
    "            driver.get(f'https://www.youtube.com/channel/{channel}/videos')\n",
    "        elif channel in ['suaradotcom']:\n",
    "            driver.get(f'https://www.youtube.com/user/{channel}/videos')\n",
    "        else:\n",
    "            driver.get(f'https://www.youtube.com/c/{channel}/videos')\n",
    "        sc = 0\n",
    "        while True:\n",
    "            time.sleep(2)\n",
    "            sc += 1080\n",
    "            driver.execute_script(\"window.scrollTo(0, {})\".format(sc))\n",
    "            date_check = WebDriverWait(driver, 60).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'#metadata-line span+span')))[-1].text\n",
    "            time.sleep(1)\n",
    "            print(date_check)\n",
    "\n",
    "            if date_check.split()[1] == 'days':\n",
    "                time_passed = round(time.time() - start_time)\n",
    "                print(f\"{time_passed} seconds passed after crawling title in channel {decode}\")\n",
    "                break\n",
    "\n",
    "            if date_check in days_list:\n",
    "                time_passed = round(time.time() - start_time)\n",
    "                print(f\"{time_passed} seconds passed after crawling title in channel {decode}\")\n",
    "                break\n",
    "\n",
    "        soup = bs(driver.page_source, 'lxml')\n",
    "        links_html = soup.select('#video-title')\n",
    "\n",
    "        for i in links_html:\n",
    "            links.append(i.get('href'))\n",
    "            channel_name.append(decode)\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    print(f'\\nStarting to get all data, total: {len(links)}...=================================================================\\n')        \n",
    "    bot_msg_hafidz(f'Starting to get all data, total: {len(links)}')\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    \n",
    "    from youtube_transcript_api import YouTubeTranscriptApi # API for caption\n",
    "    from googleapiclient.discovery import build # API for comment\n",
    "\n",
    "    import youtube_dl # API for downloading mp3\n",
    "    import re\n",
    "\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    import time\n",
    "    from pytube import YouTube # API for description\n",
    "\n",
    "    import urllib.request\n",
    "    import json\n",
    "    import urllib\n",
    "\n",
    "    driver_wait = 5\n",
    "\n",
    "\n",
    "    youtube = build('youtube','v3', developerKey=\"AIzaSyAe_QJ3KaRn45vYQ-QxSEQEqoiTUkpBh3E\")\n",
    "\n",
    "    for link, ch in zip(tqdm(links),channel_name):\n",
    "        try:\n",
    "            driver.get(f\"https://www.youtube.com/{link}\")\n",
    "        except:\n",
    "            print('driver problem')\n",
    "        time.sleep(2)\n",
    "\n",
    "        each_data = {}\n",
    "        each_data.update({\"video_link\": f\"https://www.youtube.com{link}\"})\n",
    "        try:\n",
    "            params = {\"format\": \"json\", \"url\": link}\n",
    "            url = \"https://www.youtube.com/oembed\"\n",
    "            query_string = urllib.parse.urlencode(params)\n",
    "            url = url + \"?\" + query_string\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                response_text = response.read()\n",
    "                data = json.loads(response_text.decode())\n",
    "                video_title = (data['title'])\n",
    "            each_data.update({\"video_title\": video_title})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            each_data.update({\"video_title\": '-'})\n",
    "        try: \n",
    "            video_date = WebDriverWait(driver, driver_wait).until(EC.presence_of_element_located((By.CSS_SELECTOR,'#info-strings'))).text\n",
    "            each_data.update({\"video_date\": video_date})\n",
    "            print(video_date)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            each_data.update({\"video_date\": '-'})\n",
    "        try:\n",
    "            view_count = WebDriverWait(driver, driver_wait).until(EC.presence_of_element_located((By.CSS_SELECTOR,'.view-count'))).text\n",
    "            each_data.update({\"view_count\": view_count})\n",
    "            print(view_count)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            each_data.update({\"view_count\": '-'})\n",
    "        try:\n",
    "            like_count = WebDriverWait(driver, driver_wait).until(EC.presence_of_element_located((By.CSS_SELECTOR,'.yt-simple-endpoint>yt-formatted-string'))).text\n",
    "            each_data.update({\"like_count\": like_count})\n",
    "            print(like_count)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            each_data.update({\"like_count\": '-'})\n",
    "        try:\n",
    "            video_desc = YouTube(link).description\n",
    "            #video_desc = WebDriverWait(driver, driver_wait).until(EC.presence_of_element_located((By.XPATH,'/html/body/ytd-app/div/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[9]/div[2]/ytd-video-secondary-info-renderer/div/ytd-expander/div/div/yt-formatted-string'))).text\n",
    "            each_data.update({\"video_desc\": video_desc})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            each_data.update({\"video_desc\": '-'})\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            transcriptList = YouTubeTranscriptApi.list_transcripts(re.findall(r'(?<=watch\\?v\\=).+', link)[0])\n",
    "            dataCaption = []\n",
    "            for transcript in transcriptList:\n",
    "                for text in transcript.translate('id').fetch():\n",
    "                    dataCaption.append(text.get('text'))\n",
    "            data_caption_str = ''\n",
    "            for i in dataCaption:\n",
    "                data_caption_str += i + \" \"\n",
    "            each_data.update({\"caption\": data_caption_str})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            each_data.update({\"caption\": '-'})\n",
    "\n",
    "        try:\n",
    "            video_response = youtube.commentThreads().list(part='snippet,replies',videoId=re.findall(r'(?<=watch\\?v\\=).+', link)[0]).execute()\n",
    "            video_comment = []\n",
    "            comment_dict = {\n",
    "                \"authorDisplayName\": '',\n",
    "                \"authorChannelUrl\": '',\n",
    "                \"authorProfileImageUrl\": '',\n",
    "                \"textOriginal\": '',\n",
    "                \"likeCount\": ''\n",
    "            }\n",
    "            for i in video_response['items']:\n",
    "                ds = i['snippet']['topLevelComment']['snippet']\n",
    "                for com_key in comment_dict.keys():\n",
    "                    comment_dict[com_key] = ds[com_key]\n",
    "                video_comment.append(comment_dict.copy())\n",
    "            each_data.update({\"video_comment\": video_comment})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            each_data.update({\"video_comment\": '-'})\n",
    "\n",
    "        each_data.update({\"channel_name\": ch})\n",
    "\n",
    "        all_data.append(each_data)\n",
    "        \n",
    "    all_data_table = pd.DataFrame(all_data)\n",
    "    no_caption_df = all_data_table[all_data_table['caption'] == '-']\n",
    "    \n",
    "    bot_msg_hafidz(f'There is {len(all_data_table) - len(no_caption_df)}/{len(all_data_table)} data!')\n",
    "\n",
    "\n",
    "\n",
    "    no_stt = f'data_yt_harian_{date_time}_raw.json'\n",
    "\n",
    "    with open(no_stt,'w') as f:\n",
    "        all_data = json.dump(all_data_table.to_dict('records'),f)\n",
    "    no_caption_df = all_data_table[all_data_table['caption'] == '-']\n",
    "    \n",
    "    print('\\nStarting to download the remaining mp3 caption...============================================\\n')\n",
    "    bot_msg_hafidz('Starting to download the remaining mp3 caption...')\n",
    "    \n",
    "    import youtube_dl\n",
    "\n",
    "\n",
    "\n",
    "    for each_link in tqdm(no_caption_df['video_link'].to_list()):\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }],\n",
    "            'outtmpl': 'data01/' + each_link.split('/www.youtube.com/')[1] + '.mp3'\n",
    "        }\n",
    "        try:\n",
    "            with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "                dictMeta = ydl.extract_info(each_link,download=False)\n",
    "                if dictMeta['duration'] < 1800:\n",
    "                    ydl.download([each_link])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    print('Starting speech to text...=================================================================')\n",
    "    bot_msg_hafidz('Starting speech to text...')\n",
    "\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "\n",
    "    mypath = 'C:/Users/hafid/main/projects/data_science/youtube_scraping/data01'\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    onlyfiles = ['data01/'+i for i in onlyfiles]\n",
    "    onlyfiles = [i for i in onlyfiles if '.mp3' in i]\n",
    "    \n",
    "#     print(onlyfiles[0])\n",
    "    \n",
    "    import os\n",
    "    import speech_to_text\n",
    "    stt = []\n",
    "    for each_file in tqdm(onlyfiles):\n",
    "        try:\n",
    "            stt_out = speech_to_text.parse_transcription(each_file)\n",
    "            print(stt_out)\n",
    "            stt.append(stt_out)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    df = all_data_table\n",
    "    import re\n",
    "    for i in stt:\n",
    "        if i['transcription']:\n",
    "            locate_link = df['video_link'] == 'https://www.youtube.com/' + i['file'].split('.mp3')[0].replace('#','?')\n",
    "            df.loc[locate_link,['caption']] = i['transcription']\n",
    "    df.drop(df[df['caption'] == '-' ].index, inplace=True)\n",
    "    \n",
    "    bot_msg_hafidz(f'There is {len(df)}/{len(all_data_table)} data after speech to text!')\n",
    "    \n",
    "    save = f'data_yt_harian_{date_time}.json'\n",
    "\n",
    "    with open(save,'w') as f:\n",
    "        json.dump(df.to_dict('records'),f)\n",
    "        \n",
    "    bot_msg_hafidz(save)\n",
    "    \n",
    "    import os\n",
    "    dir = 'C:/Users/hafid/main/projects/data_science/youtube_scraping/data01'\n",
    "    for f in os.listdir(dir):\n",
    "        try:\n",
    "            os.remove(os.path.join(dir, f))\n",
    "        except:\n",
    "            pass\n",
    "    print('Done=================================================================')\n",
    "#     from IPython.display import clear_output\n",
    "#     clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smooth-entertainment",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-17T04:44:55.249Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "schedule.every().day.at(\"23:59\").do(crawling_harian)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-kentucky",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-theory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
